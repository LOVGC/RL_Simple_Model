{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utility import *\n",
    "\n",
    "from scipy.fft import fft, ifft, fftfreq, fftshift\n",
    "import scipy.signal\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.agents import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the radar environment\n",
    "class RadarEnvironment(Environment):\n",
    "    def __init__(self, basis_freqs):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            basis_freqs ([type] Nx1 numpy array): [description] contains N basis angular freqs\n",
    "            max_timesteps ([type] int ): [description] the maximum number of timesteps per episode\n",
    "        \"\"\"\n",
    "\n",
    "        # define environment hyperparameters (in this case, the hyperparameters are the radar waveform set parameterization)\n",
    "        self.basis_freqs = basis_freqs\n",
    "        self.N = self.basis_freqs.size\n",
    "        self.band_width = np.max(self.basis_freqs) - np.min(self.basis_freqs)\n",
    "        self.Fs = 2 * self.band_width  # sampling frequency for the radar waveform\n",
    "        # duration is two periods of the exponential with the smallest freq\n",
    "        self.duration = (0, 2 * 1.0 / np.min(np.abs(self.basis_freqs)))\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def states(self):\n",
    "        return dict(type='float', shape=(self.N, 1), min_value=-1.0, max_value=1.0)\n",
    "\n",
    "    # Q: does action have to be discrete??\n",
    "    def actions(self):\n",
    "        # 0: -0.1, 1: 0, 2: 0.1\n",
    "        return dict(type='int', shape=(self.N, 1), num_actions =3)\n",
    "\n",
    "    def action_map(self, actions):\n",
    "        \"\"\"[summary] 0: -0.1, 1: 0, 2: 0.1\n",
    "            e.g. \n",
    "\n",
    "            actions repr used by agent: [0 1 2 1 1].T --> actions repr used by the environment [-0.1 0 0.1 0 0].T\n",
    "\n",
    "        Args:\n",
    "            actions ([type]): [description] the actions repr used by the agent\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description] the actions repr used by the environment\n",
    "        \"\"\"\n",
    "        return -0.1 + 0.1 * actions\n",
    "\n",
    "    def max_episode_timesteps(self):\n",
    "        return super().max_episode_timesteps()\n",
    "\n",
    "    def close(self):\n",
    "        super().close()\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.current_state = np.random.uniform(\n",
    "            low=-1.0, high=1.0, size=(self.N, 1))\n",
    "\n",
    "        return self.current_state\n",
    "\n",
    "    #####################################################################\n",
    "    # Utility functions for computing waveform parameters\n",
    "    #####################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def freqD_to_timeD(state, duration, basis_freqs, Fs):\n",
    "        \"\"\"[summary] convert the waveform parameters to its time domain representation\n",
    "\n",
    "        Args:\n",
    "            state ([type]): [description] a column vector (2D np array with shape (N, 1))\n",
    "            duration ([type] a tuple (start_time, end_time)): [description]\n",
    "            basis_freqs ([type] column vector): [description]\n",
    "            BW ([type] scalar): [description]\n",
    "            Fs ([type] scalar): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description] p_b is a row vector, a row vector is a 2-D np array with shape(1, N)\n",
    "        \"\"\"\n",
    "\n",
    "        w = 2*np.pi*basis_freqs\n",
    "\n",
    "        a = state.T\n",
    "        t = np.arange(duration[0], duration[1], 1.0/Fs).reshape((1, -1))\n",
    "\n",
    "        A = w @ t\n",
    "\n",
    "        p_b = a @ np.exp(1j * A)  # here p_b is a row vector\n",
    "\n",
    "        return p_b\n",
    "\n",
    "    @staticmethod\n",
    "    def point_spread_fn(p_b, Fs):\n",
    "        \"\"\"[summary] assume p_b is a row vector. \n",
    "            convert the p_b, a time domain repr of the baseband signal, into its point spread function, \n",
    "            a complex-valued function\n",
    "\n",
    "        Args:\n",
    "            p_b ([type]): [description] a row vector\n",
    "            Fs ([type]): [description] sampling frequency\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description] (t, psf): all row vectors\n",
    "        \"\"\"\n",
    "        (freqs, spectrum) = fourier_spectrum(p_b, Fs)\n",
    "        psd = np.abs(spectrum) ** 2\n",
    "        psf = fftshift(ifft(psd))\n",
    "\n",
    "        N = psf.size\n",
    "        t = np.linspace(0, N * 1.0 / Fs, N).reshape((1, -1))\n",
    "        return t, psf\n",
    "\n",
    "    @staticmethod\n",
    "    def peak_sidelobe_level(psf):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            psf ([type] row vector): [description] row vector with shape (1, N)\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description] peak sidelobe level, which is a negative real number.\n",
    "                    -inf means the best, and 0 means the worst.\n",
    "        \"\"\"\n",
    "        psf_abs = np.abs(psf)\n",
    "        psf_abs_sorted = np.sort(psf_abs)\n",
    "\n",
    "        PSL = 20 * np.log10(psf_abs_sorted[0, -2] / psf_abs_sorted[0, -1])\n",
    "        return PSL\n",
    "\n",
    "    @staticmethod\n",
    "    def main_lobe_width(state):\n",
    "        pass\n",
    "\n",
    "    ################################################################\n",
    "    # methods that defines the environment dynamics\n",
    "    ###################################################################\n",
    "\n",
    "    # define helper functions used by execute()\n",
    "    def state_transition(self, actions):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            actions ([type]): [description] At, action repr used by the agent\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description] S_(t+1), the next_state of the environment\n",
    "        \"\"\"\n",
    "        actions = self.action_map(\n",
    "            actions)  # convert action repr used by agent to action repr used by envrionment\n",
    "        next_state = self.current_state + actions\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def compute_reward(self, state):\n",
    "        \"\"\"[summary] Compute the reward associated with the inpute state\n",
    "\n",
    "        Args:\n",
    "            state ([type]): [description]\n",
    "        \"\"\"\n",
    "        p_b = RadarEnvironment.freqD_to_timeD(\n",
    "            state, self.duration, self.basis_freqs, self.Fs)\n",
    "        (t, psf) = RadarEnvironment.point_spread_fn(p_b, Fs)\n",
    "        PSL = RadarEnvironment.peak_sidelobe_level(\n",
    "            psf)  # recall it is always a negative scalar\n",
    "\n",
    "        reward = -PSL\n",
    "        return reward\n",
    "\n",
    "    def dynamics_fn(self, actions):\n",
    "        \"\"\"The dynamics_fn is a blackbox whose input is current_state and action, \n",
    "        In this implementation, the current_state is contained in \"self\", thus, we don't \n",
    "        explicitly inlude current_state as input.\n",
    "\n",
    "        Args:\n",
    "            actions ([type] N by 1 integer arrays): [description] action repr used by the agent\n",
    "\n",
    "        Returns:\n",
    "            (next_state, reward)\n",
    "        \"\"\"\n",
    "\n",
    "        # define helper functions for computing reward: recall the reward R_(t+1) is due to A_t and S_t,\n",
    "        # thus R_(t + 1) is associated with the reward for state S_(t + 1)\n",
    "\n",
    "        next_state = self.state_transition(actions)\n",
    "        reward = self.compute_reward(next_state)\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    def execute(self, actions):\n",
    "\n",
    "        # increment timestep\n",
    "        self.timestep += 1\n",
    "\n",
    "        # update the current_state\n",
    "        self.current_state, self.reward = self.dynamics_fn(actions)\n",
    "\n",
    "        terminal = False\n",
    "\n",
    "        return self.current_state, terminal, self.reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test area\n",
    "N = 10\n",
    "max_episode_length = 100\n",
    "basis_freqs_10 = np.random.uniform(low=-100, high=100, size=(N, 1))\n",
    "\n",
    "\n",
    "## Creation of the environment via Environment.create() creates\n",
    "## a wrapper class around the original Environment defined here.\n",
    "## That wrapper mainly keeps track of the number of timesteps.\n",
    "## Environment.create() returns an instance of the wrapped version of your original environment class.\n",
    "\n",
    "radar_env = Environment.create(environment=RadarEnvironment,\n",
    "                               max_episode_timesteps=max_episode_length, basis_freqs=basis_freqs_10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-53b6afb6c90b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# it includes the discount factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m agent = Agent.create(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tensorforce'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mradar_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(agent, environment, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# Keyword specification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(agent, environment, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/agents/tensorforce.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, update, optimizer, objective, reward_estimation, max_episode_timesteps, policy, memory, baseline, baseline_optimizer, baseline_objective, l2_regularization, entropy_regularization, state_preprocessing, exploration, variable_noise, parallel_interactions, config, saver, summarizer, tracking, recorder, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m#     reward_estimation['horizon'] = max_episode_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mparallel_interactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, max_episode_timesteps, parallel_interactions, config, recorder)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mfn_act\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mmax_episode_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/agents/recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn_act, states, actions, max_episode_timesteps, parallel_interactions, recorder)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorsSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'type'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'shape'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorsSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingleton\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorsSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/core/utils/tensors_spec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, singleton, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingleton\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingleton\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingleton\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/core/utils/nested_dict.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg, value_type, overwrite, singleton, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTensorforceError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NestedDict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'singleton'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/core/utils/tensors_spec.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTensorforceError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TensorsSpec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m'type'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'shape'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorsSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_actions'"
     ]
    }
   ],
   "source": [
    "# check the tensorforce agent specs: https://tensorforce.readthedocs.io/en/latest/agents/tensorforce.html\n",
    "# it includes the discount factor\n",
    "\n",
    "agent = Agent.create(\n",
    "    agent='tensorforce', environment=radar_env, update=64,\n",
    "    optimizer=dict(optimizer='adam', learning_rate=1e-3),\n",
    "    objective='policy_gradient', reward_estimation=dict(horizon=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Untrained Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize\n",
    "state = radar_env.reset()\n",
    "\n",
    "internals = agent.initial_internals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3239e8400f967b912aa5ff52c1d70244e44ae8ab469e58fe9dd95ca3971341f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
